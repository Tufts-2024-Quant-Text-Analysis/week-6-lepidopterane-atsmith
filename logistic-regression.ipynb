{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression â€” Brezina ch. 4 + Google's Crash Course in Machine Learning\n",
    "\n",
    "Because Brezina's explanations are sparse (no pun intended), we're going to lean on Google's Crash Course in Machine Learning to talk about Logistic Regression.\n",
    "\n",
    "But first, we'll need to look at [linear regression](https://developers.google.com/machine-learning/crash-course/linear-regression).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Linear Regression\n",
    "\n",
    "https://developers.google.com/machine-learning/crash-course/linear-regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add as many cells as you need"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  linear regression for beginners\n",
    "-_y = mx + b_ is _y' = w(1)x(1) + b_ where:\n",
    "==> _b_ is y-intercept in the former and bias in the model\n",
    "==> _w(1)_ is much like the slope _m_ where it's like slope, but we call it weight.\n",
    "==> _x_ and _y_, simply put, are our input and output. they sometimes refer to these as features and predictions.\n",
    "- \"During training, the model calculates the weight and bias that produce the best model.\"\n",
    "- models can have a lot of features, not just the one. and weight of different kinds with each feature. this is reminding me of Shakespeare Machine and what they do, it's actually starting to click for me yay\n",
    "\n",
    "## loss\n",
    "- referring here to the numerical metric that describes how wrong a model's predictions are. I'm faintly positive this is just a stripped down version of MSE\n",
    "- aaand there we go. MSE mention. I don't feel like doing a table version of the table they have here for jupiter notes like these, so I'll just put a table screenshot in OneNote.\n",
    "- \"MSE moves the model more toward the outliers, while MAE doesn't. L2 loss incurs a much higher penalty for an outlier than L1 loss.\" so we use MSE for goodness of fit?\n",
    "\n",
    "## gradient descent\n",
    "- I think I know how this works...\n",
    "- \"The loss functions for linear models always produce a convex surface\"\n",
    "- \"It's important to note that the model almost never finds the exact minimum for each weight and bias, but instead finds a value very close to it. It's also important to note that the minimum for the weights and bias don't correspond to zero loss, only a value that produces the lowest loss for that parameter.\"\n",
    "\n",
    "## hyperhypers wheee\n",
    "- learning rate: influences how quickly the model converges. too low = too slow, too high = never converges\n",
    "==> OHO its step size of gradient descent basically\n",
    "- batch size\n",
    "==> mini-SGD keeps loss curve from being overly crunchy\n",
    "- an epoch means that the model has processed every example in the training set once.\n",
    "\n",
    "## the coding thingy \n",
    "- i did it in colab\n",
    "- \"Unless you are interested, it is not important for you to understand how these plotting functions work.\" its plotly!\n",
    "- \"The topography of a simple linear regression model is a single node in a single layer.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression\n",
    "\n",
    "https://developers.google.com/machine-learning/crash-course/logistic-regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# again, add code cells and experiment as needed\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## intro\n",
    "- i now know what a sigmoid is\n",
    "- i now know why it's logistical regression. weights biases equation in the z of 1/(1 + e^(-z))\n",
    "\n",
    "## loss n regularization\n",
    "- Applying regularization is critical to prevent overfitting.\n",
    "- and now this whole thing is done"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
